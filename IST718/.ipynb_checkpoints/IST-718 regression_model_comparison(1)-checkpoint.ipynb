{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading preliminary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "checksum": "c51f80b694da894627ba37be28c86055",
     "grade": false,
     "grade_id": "cell-b038e38b5e3072a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "from pyspark.sql import SQLContext\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "checksum": "c51f80b694da894627ba37be28c86055",
     "grade": false,
     "grade_id": "cell-b038e38b5e3072a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to determine if we are running on data bricks\n",
    "# Return true if running in the data bricks environment, false otherwise\n",
    "import os\n",
    "def is_databricks():\n",
    "    if os.getenv(\"DATABRICKS_RUNTIME_VERSION\") != None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define a function to read the data. The full path name is constructed by checking\n",
    "# the runtime environment to determine if it is databricks or a personal computer.\n",
    "# On the local filesystem the data is assumed to be in the same directory as the code.\n",
    "# On databricks, the data path is assumed to be at '/FileStore/tables/' location.\n",
    "# \n",
    "# Parameter(s):\n",
    "#   name: The base name of the file, parquet file or parquet directory\n",
    "# Return Value:\n",
    "#   full_path_name: the full path name of the data based on the runtime environment\n",
    "#\n",
    "# Correct Usage Example (pass ONLY the full file name):\n",
    "#   name_to_load = get_datapath(\"sms_spam.csv\") # correct  \n",
    "#   \n",
    "# Incorrect Usage Examples:\n",
    "#   name_to_load = get_datapath(\"/sms_spam.csv\") # incorrect\n",
    "#   name_to_load = get_datapath(\"sms_spam.csv/\") # incorrect\n",
    "#   name_to_load = get_datapath(\"c:/users/will/data/sms_spam.csv\") incorrect\n",
    "#\n",
    "def get_datapath(name):    \n",
    "    if is_databricks():\n",
    "        full_path_name = \"/FileStore/tables/%s\" % name\n",
    "    else:\n",
    "        full_path_name = name\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>map</th>\n",
       "      <th>tc</th>\n",
       "      <th>ldl</th>\n",
       "      <th>hdl</th>\n",
       "      <th>tch</th>\n",
       "      <th>ltg</th>\n",
       "      <th>glu</th>\n",
       "      <th>disease_progression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>32.1</td>\n",
       "      <td>101.0</td>\n",
       "      <td>157</td>\n",
       "      <td>93.2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.8598</td>\n",
       "      <td>87</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>21.6</td>\n",
       "      <td>87.0</td>\n",
       "      <td>183</td>\n",
       "      <td>103.2</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.8918</td>\n",
       "      <td>69</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>30.5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>156</td>\n",
       "      <td>93.6</td>\n",
       "      <td>41.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.6728</td>\n",
       "      <td>85</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>25.3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>198</td>\n",
       "      <td>131.4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.8903</td>\n",
       "      <td>89</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>192</td>\n",
       "      <td>125.4</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.2905</td>\n",
       "      <td>80</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex   bmi    map   tc    ldl   hdl  tch     ltg  glu  \\\n",
       "0   59    2  32.1  101.0  157   93.2  38.0  4.0  4.8598   87   \n",
       "1   48    1  21.6   87.0  183  103.2  70.0  3.0  3.8918   69   \n",
       "2   72    2  30.5   93.0  156   93.6  41.0  4.0  4.6728   85   \n",
       "3   24    1  25.3   84.0  198  131.4  40.0  5.0  4.8903   89   \n",
       "4   50    1  23.0  101.0  192  125.4  52.0  4.0  4.2905   80   \n",
       "\n",
       "   disease_progression  \n",
       "0                  151  \n",
       "1                   75  \n",
       "2                  141  \n",
       "3                  206  \n",
       "4                  135  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use pandas to display the data nicely\n",
    "import pandas as pd\n",
    "# datasource = pd.read_csv('diabetes_tab.txt', delimiter='\\t')\n",
    "diabetes = spark.read.csv(get_datapath('diabetes*csv'), header=True, inferSchema=True)\n",
    "datasource = diabetes.toPandas()\n",
    "datasource.columns = ['age', 'sex', 'bmi', 'map', 'tc', 'ldl', 'hdl', 'tch', 'ltg', 'glu', 'disease_progression']\n",
    "datasource.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a spark dataframe from the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df = spark.createDataFrame(datasource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the spark dataframe schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diabetes_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 5 rows of the spark data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at age and compute several statistics in the population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.select(fn.count('*'), fn.avg('age'), fn.min('age'), fn.max('age'), fn.stddev('age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several functions in `functions` to compute other statistics. For example, we can compute the correlation between two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.select(fn.corr('bmi', 'disease_progression')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can collect any Spark dataframe into a Pandas dataframe by using the method `toPandas()`. However, you have to be very careful because this will collect all the information from the workers and push the data to the driver. If the driver does not have enough memory to hold a big dataset, then Spark might crash. So before running `toPandas()` make sure that the Spark dataframe you are collecting is relatively small**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we can take a look at the distribution of things by using the `describe` method in a Spark dataframe and then collecting it into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we know that we only have 442 observations so we can collect the data easily. However, sometimes your dataset is too big so you might want to *sample* from it.\n",
    "\n",
    "There is a method called `sample(with_replacement, fraction)` that allows you to do that. `with_replacement` indicates whether you will replace back samples that you already sampled and `fraction` indicates the fraction of rows that you will sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a 0.1 sample will give you 10% of 442, which is approximately 44:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.sample(False, 0.1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, again, the diabetes dataset is not that big, so we can collect all of it into the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot `bmi` vs `disease_progression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.select('bmi', 'disease_progression').toPandas().plot(x='bmi', y='disease_progression', kind='scatter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do other kinds of plots, such as histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.select('age').toPandas().plot(kind='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear relationship between BMI and disease progression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is built on the idea that there is a linear relationship between a set of explanatory variables (or independent variables) and a dependent variable (or taget variable).\n",
    "\n",
    "$$Y = f(X) = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_m x_m$$\n",
    "\n",
    "using maximum likelihood estimation we estimate such a model by finding the values of \\\\(b_0, \\dots, b_m\\\\) that minimize the mean square error:\n",
    "\n",
    "$$\\hat{b} = \\arg \\min_b \\sum (Y_i - f(X_i))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Spark, we can find such relationship with a `LinearRegression` estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, `LinearRegression` receives as input a vector of features. The `bmi` column, however, is a scalar so we need to transform it into a vector of size one.\n",
    "\n",
    "We will do this by using an estimator that takes a set of columns and produces a single column as a vector, which combines all the input columns. This estimator is called `VectorAssembler` and it is defined inside `pyspark.ml.feature.VectorAssembler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = feature.VectorAssembler(inputCols=['bmi'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = regression.LinearRegression(featuresCol='features', labelCol='disease_progression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages=[va, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_model = pipe.fit(diabetes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_model.transform(diabetes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_model.transform(diabetes_df).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_model.stages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_model.stages[1].intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_model.stages[1].coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, validation_df, testing_df = diabetes_df.randomSplit([0.6, 0.3, 0.1], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[training_df.count(), validation_df.count(), testing_df.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1_model = Pipeline(stages=[\n",
    "  feature.VectorAssembler(inputCols=['bmi'], outputCol='features'),\n",
    "  regression.LinearRegression(featuresCol='features', labelCol='disease_progression')  \n",
    "]).fit(training_df)\n",
    "\n",
    "pipe2_model = Pipeline(stages=[\n",
    "  feature.VectorAssembler(inputCols=['bmi', 'sex', 'age'], outputCol='features'),\n",
    "  regression.LinearRegression(featuresCol='features', labelCol='disease_progression')  \n",
    "]).fit(training_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe1_model.transform(validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to compute mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = fn.mean((fn.col('disease_progression') - fn.col('prediction'))**2).alias('mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using the validation data and calculate the mse for pipe1 and pipe2.  Note that model 2 performs better because it has a lower mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe1_model.transform(validation_df).select(mse).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2_model.transform(validation_df).select(mse).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the performance of pipe 2 increases significantly when we predict using the training data.  This is not a realistic test.  Model performance should never be measured using the training data - always use unseen data (the validation data) which was not used during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2_model.transform(training_df).select(mse).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we test model 1 with the test data frame.  Note that the test data frame only has 10% of the total number of samples in the original data.  The performance improves but it's probably because there is a lower number of samples in the data.  Always use the same test or validation data set to compare results between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe1_model.transform(testing_df).select(mse).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another more sophisticated transformer that takes a *formula* equation and produces a `features` column. This *formula* equation follows the `R` format (see [PDF](http://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf)).\n",
    "\n",
    "For example, if we want to build a feature set that is the `bmi`, `age`, and the multiplication of both (interaction), then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = feature.RFormula(formula='disease_progression ~ bmi + age + bmi:age', labelCol='disease_progression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the transformation that can go inside formulas are more complex, we need to fit the estimator first, and capture the returning object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_model = formula.fit(diabetes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transform below adds the \"featues\" column to the dataframe.  We also print the 'bmi' and 'age' columns to show that the features column contains 'bmi', 'age', and the multiplication of the 2 (from the bmi:age param).  The 'disease_progression' column is the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_model.transform(diabetes_df).select('bmi', 'age', 'features', 'disease_progression').show(5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "name": "Introduction to Spark and DataFrames",
  "notebookId": 3838174134834519
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

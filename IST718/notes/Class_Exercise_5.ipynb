{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "*CLASS EXERCISE 5 (2 PTS)*\n",
    "\n",
    "## General instructions:\n",
    "\n",
    "- Class Exercises begin in class and are submitted within a day or two. Submit your .ipynb file on Blackboard.\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers either from your classmates or from the internet__\n",
    "- Remove or comment out code that contains `raise NotImplementedError`. This is mainly to make the `assert` statement fail if nothing is submitted.\n",
    "- The tests shown in some cells (i.e., `assert` and `np.testing.` statements) are used to grade your answers. **However, the professor and FAs will use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
    "- Before downloading and submitting your work through Blackboard, remember to save and press `Validate` (or go to \n",
    "`Kernel`$\\rightarrow$`Restart and Run All`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import feature\n",
    "from pyspark.ml import classification\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, \\\n",
    "    MulticlassClassificationEvaluator, \\\n",
    "    RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine if we are running on data bricks\n",
    "# Return true if running in the data bricks environment, false otherwise\n",
    "import os\n",
    "def is_databricks():\n",
    "    if os.getenv(\"DATABRICKS_RUNTIME_VERSION\") != None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define a function to read the data. The full path name is constructed by checking\n",
    "# the runtime environment to determine if it is databricks or a personal computer.\n",
    "# On the local filesystem the data is assumed to be in the same directory as the code.\n",
    "# On databricks, the data path is assumed to be at '/FileStore/tables/' location.\n",
    "# \n",
    "# Parameter(s):\n",
    "#   name: The base name of the file, parquet file or parquet directory\n",
    "# Return Value:\n",
    "#   full_path_name: the full path name of the data based on the runtime environment\n",
    "#\n",
    "# Correct Usage Example (pass ONLY the full file name):\n",
    "#   name_to_load = get_datapath(\"sms_spam.csv\") # correct  \n",
    "#   \n",
    "# Incorrect Usage Examples:\n",
    "#   name_to_load = get_datapath(\"/sms_spam.csv\") # incorrect\n",
    "#   name_to_load = get_datapath(\"sms_spam.csv/\") # incorrect\n",
    "#   name_to_load = get_datapath(\"c:/users/will/data/sms_spam.csv\") incorrect\n",
    "#\n",
    "def get_datapath(name):    \n",
    "    if is_databricks():\n",
    "        full_path_name = \"/FileStore/tables/%s\" % name\n",
    "    else:\n",
    "        full_path_name = name\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning for Titanic Survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/titanic\n",
    "\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = spark.read.csv(get_datapath('titanic_original.csv'), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Column Descriptions](https://data.world/nrippner/titanic-disaster-dataset): <br>\n",
    "survival - Survival (0 = No; 1 = Yes) <br>\n",
    "class - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) <br>\n",
    "name - Name <br>\n",
    "sex - Sex <br>\n",
    "age - Age <br>\n",
    "sibsp - Number of Siblings/Spouses Aboard <br>\n",
    "parch - Number of Parents/Children Aboard <br>\n",
    "ticket - Ticket Number <br>\n",
    "fare - Passenger Fare <br>\n",
    "cabin - Cabin <br>\n",
    "embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) <br>\n",
    "boat - Lifeboat (if survived) <br>\n",
    "body - Body number (if did not survive and body was recovered) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "      <td>135.0</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived                                             name     sex  \\\n",
       "0       1         1                    Allen, Miss. Elisabeth Walton  female   \n",
       "1       1         1                   Allison, Master. Hudson Trevor    male   \n",
       "2       1         0                     Allison, Miss. Helen Loraine  female   \n",
       "3       1         0             Allison, Mr. Hudson Joshua Creighton    male   \n",
       "4       1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
       "\n",
       "       age  sibsp  parch  ticket      fare    cabin embarked  boat   body  \\\n",
       "0  29.0000      0      0   24160  211.3375       B5        S     2    NaN   \n",
       "1   0.9167      1      2  113781  151.5500  C22 C26        S    11    NaN   \n",
       "2   2.0000      1      2  113781  151.5500  C22 C26        S  None    NaN   \n",
       "3  30.0000      1      2  113781  151.5500  C22 C26        S  None  135.0   \n",
       "4  25.0000      1      2  113781  151.5500  C22 C26        S  None    NaN   \n",
       "\n",
       "                         home.dest  \n",
       "0                     St Louis, MO  \n",
       "1  Montreal, PQ / Chesterville, ON  \n",
       "2  Montreal, PQ / Chesterville, ON  \n",
       "3  Montreal, PQ / Chesterville, ON  \n",
       "4  Montreal, PQ / Chesterville, ON  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'types:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pclass         int32\n",
       "survived       int32\n",
       "name          object\n",
       "sex           object\n",
       "age          float64\n",
       "sibsp          int32\n",
       "parch          int32\n",
       "ticket        object\n",
       "fare         float64\n",
       "cabin         object\n",
       "embarked      object\n",
       "boat          object\n",
       "body         float64\n",
       "home.dest     object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_df_pd = titanic_df.limit(10).toPandas()\n",
    "display(titanic_df_pd.head())\n",
    "display(\"types:\", titanic_df_pd.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic cleanup\n",
    "drop_cols = ['boat', 'body']\n",
    "new_titanic_df = titanic_df.\\\n",
    "    drop(*drop_cols).\\\n",
    "    withColumnRenamed('home.dest', 'home_dest') .\\\n",
    "    dropna(subset=['pclass', 'age', 'sibsp', 'parch', 'fare', 'survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+---+---+-----+-----+------+----+-----+--------+---------+\n",
      "|pclass|survived|name|sex|age|sibsp|parch|ticket|fare|cabin|embarked|home_dest|\n",
      "+------+--------+----+---+---+-----+-----+------+----+-----+--------+---------+\n",
      "|     0|       0|   0|  0|  0|    0|    0|     0|   0|  773|       2|      360|\n",
      "+------+--------+----+---+---+-----+-----+------+----+-----+--------+---------+\n",
      "\n",
      "+------+--------+----+---+---+-----+-----+------+----+-----+--------+---------+\n",
      "|pclass|survived|name|sex|age|sibsp|parch|ticket|fare|cabin|embarked|home_dest|\n",
      "+------+--------+----+---+---+-----+-----+------+----+-----+--------+---------+\n",
      "|     0|       0|   0|  0|  0|    0|    0|     0|   0|    0|       0|        0|\n",
      "+------+--------+----+---+---+-----+-----+------+----+-----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, isnull, when, count, col\n",
    "\n",
    "new_titanic_df.select([count(when(isnull(c), c)).alias(c) for c in new_titanic_df.columns]).show()\n",
    "new_titanic_df.select([count(when(isnan(c), c)).alias(c) for c in new_titanic_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------------------+------+----+-----+-----+--------+------+-------+--------+--------------------+\n",
      "|pclass|survived|                name|   sex| age|sibsp|parch|  ticket|  fare|  cabin|embarked|           home_dest|\n",
      "+------+--------+--------------------+------+----+-----+-----+--------+------+-------+--------+--------------------+\n",
      "|     1|       0|\"Lindeberg-Lind, ...|  male|42.0|    0|    0|   17475| 26.55|   null|       S|   Stockholm, Sweden|\n",
      "|     1|       0|\"Rosenshine, Mr. ...|  male|46.0|    0|    0|PC 17585|  79.2|   null|       C|        New York, NY|\n",
      "|     1|       0|Allison, Miss. He...|female| 2.0|    1|    2|  113781|151.55|C22 C26|       S|Montreal, PQ / Ch...|\n",
      "|     1|       0|Allison, Mr. Huds...|  male|30.0|    1|    2|  113781|151.55|C22 C26|       S|Montreal, PQ / Ch...|\n",
      "|     1|       0|Allison, Mrs. Hud...|female|25.0|    1|    2|  113781|151.55|C22 C26|       S|Montreal, PQ / Ch...|\n",
      "+------+--------+--------------------+------+----+-----+-----+--------+------+-------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training, test = new_titanic_df.randomSplit([0.8, 0.2], 0)\n",
    "training.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classic pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a logistic regression pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the logistic regression model on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to modify the pipeline to add \"sex\" (gender) as a feature, we need to modify the point of entry and the next transformation.  The StringIndexer converts labels where the label with the highest count is label 0, next highest is label 1, etc.  See the spark documentation for [StringIndexer](https://spark.apache.org/docs/latest/ml-features.html#stringindexer) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "model1 = Pipeline(stages=[feature.VectorAssembler(inputCols=['pclass', 'age', 'sibsp', 'parch', 'fare'], \n",
    "                                                  outputCol='features'),\n",
    "                          feature.StringIndexer(inputCol='sex', outputCol='encoded_sex'),\n",
    "                          feature.VectorAssembler(inputCols=['features', 'encoded_sex'], outputCol='final_features'),\n",
    "                 LogisticRegression(labelCol='survived', featuresCol='final_features')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1_fitted = model1.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8341750841750837"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol='survived')\n",
    "evaluator.evaluate(model1_fitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decision tree pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Spark ML for DT classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a decision tree pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the decision tree model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classifier\n",
    "# define model2\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "model2= Pipeline(stages=[feature.VectorAssembler(inputCols=['pclass', 'age', 'sibsp', 'parch', 'fare'], \n",
    "                                                  outputCol='features'),\n",
    "                          feature.StringIndexer(inputCol='sex', outputCol='encoded_sex'),\n",
    "                          feature.VectorAssembler(inputCols=['features', 'encoded_sex'], outputCol='final_features'),\n",
    "                  DecisionTreeClassifier(labelCol='survived', featuresCol='final_features')])\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define model2_fitted\n",
    "\n",
    "# YOUR CODE HERE\n",
    "model2_fitted = model2.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7702231848573313"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "# YOUR CODE HERE\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='survived')\n",
    "evaluator.evaluate(model2_fitted.transform(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random forest pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Spark ML for RF classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random forest pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the random forest model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# define model3\n",
    "model3= Pipeline(stages=[feature.VectorAssembler(inputCols=['pclass', 'age', 'sibsp', 'parch', 'fare'], \n",
    "                                                  outputCol='features'),\n",
    "                          feature.StringIndexer(inputCol='sex', outputCol='encoded_sex'),\n",
    "                          feature.VectorAssembler(inputCols=['features', 'encoded_sex'], outputCol='final_features'),\n",
    "                  RandomForestClassifier(labelCol='survived', featuresCol='final_features')])\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define model3_fitted\n",
    "\n",
    "# YOUR CODE HERE\n",
    "model3_fitted = model3.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "# YOUR CODE HERE\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='survived')\n",
    "evaluator.evaluate(model3_fitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient-boosted tree pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Spark ML for GBT classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a gradient-boosted tree pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the gradient-boosted treen model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classifier\n",
    "# define model4\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "# YOUR CODE HERE\n",
    "model4 = Pipeline(stages=[feature.VectorAssembler(inputCols=['pclass', 'age', 'sibsp', 'parch', 'fare'], \n",
    "                                                  outputCol='features'),\n",
    "                          feature.StringIndexer(inputCol='sex', outputCol='encoded_sex'),\n",
    "                          feature.VectorAssembler(inputCols=['features', 'encoded_sex'], outputCol='final_features'),\n",
    "                  GBTClassifier(labelCol='survived', featuresCol='final_features')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define model4_fitted\n",
    "\n",
    "# YOUR CODE HERE\n",
    "model4_fitted = model4.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7696140246130725"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "# YOUR CODE HERE\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='survived')\n",
    "evaluator.evaluate(model4_fitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive bayes pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Spark ML for NB classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a naive bayes pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the naive bayes model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "model5 = Pipeline(stages=[feature.VectorAssembler(inputCols=['pclass', 'age', 'sibsp', 'parch', 'fare'], \n",
    "                                                  outputCol='features'),\n",
    "                          feature.StringIndexer(inputCol='sex', outputCol='encoded_sex'),\n",
    "                          feature.VectorAssembler(inputCols=['features', 'encoded_sex'], outputCol='final_features'),\n",
    "                 NaiveBayes(labelCol='survived', featuresCol='final_features')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model5_fitted = model5.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6628363249052904"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model5_fitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## support vector machines pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Spark ML for SVC classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a support vector machines pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the support vector machines model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# define model6\n",
    "model6 = Pipeline(stages=[feature.VectorAssembler(inputCols=['pclass', 'age', 'sibsp', 'parch', 'fare'], \n",
    "                                                  outputCol='features'),\n",
    "                          feature.StringIndexer(inputCol='sex', outputCol='encoded_sex'),\n",
    "                          feature.VectorAssembler(inputCols=['features', 'encoded_sex'], outputCol='final_features'),\n",
    "                 LinearSVC(labelCol='survived', featuresCol='final_features')])\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define model6_fitted\n",
    "\n",
    "# YOUR CODE HERE\n",
    "model6_fitted = model6.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7621830324533027"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "# YOUR CODE HERE\n",
    "evaluator.evaluate(model6_fitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inspect all the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model seems to be the best for the titatnic survival model? Substantiate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression pipeline model produces the best resulting by comparing all the results from the evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END OF EXERCISE*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "feature_engineering_titanic",
  "notebookId": 4503310255797310
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
